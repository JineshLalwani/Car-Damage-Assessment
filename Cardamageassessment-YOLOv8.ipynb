{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üèóÔ∏èImport Necessary Libraries\n---","metadata":{}},{"cell_type":"code","source":"# Pip install method (recommended)\n%pip install ultralytics\n!pip install ultralytics","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-01T05:52:40.665032Z","iopub.execute_input":"2023-11-01T05:52:40.665606Z","iopub.status.idle":"2023-11-01T05:53:12.208077Z","shell.execute_reply.started":"2023-11-01T05:52:40.665574Z","shell.execute_reply":"2023-11-01T05:53:12.20662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the required libraries\nfrom ultralytics import YOLO\nimport squarify\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport random\nimport pandas as pd\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nsns.set_style('darkgrid')\n\n%matplotlib inline","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679731785472,"user":{"displayName":"Umar Saeed","userId":"03898094718291107188"},"user_tz":-300},"id":"45SFExkuDsM_","execution":{"iopub.status.busy":"2023-11-01T05:53:46.105082Z","iopub.execute_input":"2023-11-01T05:53:46.106486Z","iopub.status.idle":"2023-11-01T05:53:46.114514Z","shell.execute_reply.started":"2023-11-01T05:53:46.106443Z","shell.execute_reply":"2023-11-01T05:53:46.113385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üóÇÔ∏èDataset Overview\n---","metadata":{"id":"8GOu2wG7DKvz"}},{"cell_type":"markdown","source":"## Visualizing Sample Images with Corresponding Annotations","metadata":{"id":"m-JemePzDQdS"}},{"cell_type":"code","source":"# Define the paths to the images and labels directories\ntrain_images = \"/kaggle/input/car-damage-detection/train/images\"\ntrain_labels = \"/kaggle/input/car-damage-detection/train/labels\"\n\ntest_images = \"/kaggle/input/car-damage-detection/test/images\"\ntest_labels = \"/kaggle/input/car-damage-detection/test/labels\"\n\nval_images = \"/kaggle/input/car-damage-detection/valid/images\"\nval_labels = \"/kaggle/input/car-damage-detection/valid/labels\"\n\n# Get a list of all the image files in the training images directory\nimage_files = os.listdir(train_images)\n\n# Choose 16 random image files from the list\nrandom_images = random.sample(image_files, 16)\n\n# Set up the plot\nfig, axs = plt.subplots(4, 4, figsize=(16, 16))\n\n# Loop over the random images and plot the object detections\nfor i, image_file in enumerate(random_images):\n    row = i // 4\n    col = i % 4\n    \n    # Load the image\n    image_path = os.path.join(train_images, image_file)\n    image = cv2.imread(image_path)\n\n    # Load the labels for this image\n    label_file = os.path.splitext(image_file)[0] + \".txt\"\n    label_path = os.path.join(train_labels, label_file)\n    with open(label_path, \"r\") as f:\n        labels = f.read().strip().split(\"\\n\")\n\n    # Loop over the labels and plot the object detections\n    # Loop over the labels and plot the object detections\n    for label in labels:\n        if len(label.split()) != 5:\n            continue\n        class_id, x_center, y_center, width, height = map(float, label.split())\n        x_min = int((x_center - width/2) * image.shape[1])\n        y_min = int((y_center - height/2) * image.shape[0])\n        x_max = int((x_center + width/2) * image.shape[1])\n        y_max = int((y_center + height/2) * image.shape[0])\n        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 3)\n\n\n    # Show the image with the object detections\n    axs[row, col].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    axs[row, col].axis('off')\n\nplt.show()","metadata":{"executionInfo":{"elapsed":10902,"status":"ok","timestamp":1679636094777,"user":{"displayName":"Umar Saeed","userId":"03898094718291107188"},"user_tz":-300},"id":"vMeV7OjmGOWx","outputId":"26403713-3091-4cf7-c0d1-930cbc71f67c","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T05:53:50.811289Z","iopub.execute_input":"2023-11-01T05:53:50.812093Z","iopub.status.idle":"2023-11-01T05:53:54.650641Z","shell.execute_reply.started":"2023-11-01T05:53:50.81205Z","shell.execute_reply":"2023-11-01T05:53:54.648892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Metadata","metadata":{"id":"s0_dRCqXfjJZ"}},{"cell_type":"code","source":"# Load an image using OpenCV\nimage = cv2.imread(\"/kaggle/input/car-damage-detection/test/images/000028_jpg.rf.3232837d0673c573d9b0fedbc67b37d2.jpg\")\n\n# Get the size of the image\nheight, width, channels = image.shape\nprint(f\"The image has dimensions {width}x{height} and {channels} channels.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-01T05:54:06.310239Z","iopub.execute_input":"2023-11-01T05:54:06.310985Z","iopub.status.idle":"2023-11-01T05:54:06.341765Z","shell.execute_reply.started":"2023-11-01T05:54:06.310948Z","shell.execute_reply":"2023-11-01T05:54:06.340673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÄModel Training\n---","metadata":{"id":"uS6udhjusb97"}},{"cell_type":"code","source":" # Loading a pretrained model\nmodel = YOLO('yolov8n.pt')\n\n# Training the model\nmodel.train(data = '/kaggle/input/car-damage-detection/data.yaml',\n            epochs = 60,\n            imgsz = height,\n            seed = 42,\n            batch = 40,\n            workers = 4)","metadata":{"id":"QQorLHRn9fqR","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-01T05:54:16.215144Z","iopub.execute_input":"2023-11-01T05:54:16.215992Z","iopub.status.idle":"2023-11-01T06:46:20.443376Z","shell.execute_reply.started":"2023-11-01T05:54:16.215951Z","shell.execute_reply":"2023-11-01T06:46:20.441985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìàModel Performance\n---\n<div style=\"background-color:#f2f2f2; padding: 20px;\">\n    \n<h2>Train Box Loss:</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The train box loss metric measures the difference between the predicted bounding boxes and the actual bounding boxes of the objects in the training data. A lower box loss means that the model's predicted bounding boxes more closely align with the actual bounding boxes.</p>\n    \n<h2>Train Class Loss:</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The train class loss metric measures the difference between the predicted class probabilities and the actual class labels of the objects in the training data. A lower class loss means that the model's predicted class probabilities more closely align with the actual class labels.</p>\n\n<h2>Train DFL Loss:</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The train DFL (Dynamic Feature Learning) loss metric measures the difference between the predicted feature maps and the actual feature maps of the objects in the training data. A lower DFL loss means that the model's predicted feature maps more closely align with the actual feature maps.</p>\n    \n<h2>Metrics Precision (B):</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The metrics precision (B) metric measures the proportion of true positive detections among all the predicted bounding boxes. A higher precision means that the model is better at correctly identifying true positive detections and minimizing false positives.</p>\n    \n<h2>Metrics Recall (B):</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The metrics recall (B) metric measures the proportion of true positive detections among all the actual bounding boxes. A higher recall means that the model is better at correctly identifying all true positive detections and minimizing false negatives.</p>\n    \n<h2>Metrics mAP50 (B):</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The metrics mAP50 (B) metric measures the mean average precision of the model across different object categories, with a 50% intersection-over-union (IoU) threshold. A higher mAP50 means that the model is better at accurately detecting and localizing objects across different categories.</p>\n    \n<h2>Metrics mAP50-95 (B):</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The metrics mAP50-95 (B) metric measures the mean average precision of the model across different object categories, with IoU thresholds ranging from 50% to 95%. A higher mAP50-95 means that the model is better at accurately detecting and localizing objects across different categories with a wider range of IoU thresholds.</p>\n</div>","metadata":{"id":"kDLC79j4h-VW"}},{"cell_type":"code","source":"%matplotlib inline\n# read in the results.csv file as a pandas dataframe\ndf = pd.read_csv('/kaggle/working/runs/detect/train/results.csv')\ndf.columns = df.columns.str.strip()\n\n# create subplots using seaborn\nfig, axs = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\n\n# plot the columns using seaborn\nsns.lineplot(x='epoch', y='train/box_loss', data=df, ax=axs[0,0])\nsns.lineplot(x='epoch', y='train/cls_loss', data=df, ax=axs[0,1])\nsns.lineplot(x='epoch', y='train/dfl_loss', data=df, ax=axs[1,0])\nsns.lineplot(x='epoch', y='metrics/precision(B)', data=df, ax=axs[1,1])\nsns.lineplot(x='epoch', y='metrics/recall(B)', data=df, ax=axs[2,0])\nsns.lineplot(x='epoch', y='metrics/mAP50(B)', data=df, ax=axs[2,1])\nsns.lineplot(x='epoch', y='metrics/mAP50-95(B)', data=df, ax=axs[3,0])\nsns.lineplot(x='epoch', y='val/box_loss', data=df, ax=axs[3,1])\nsns.lineplot(x='epoch', y='val/cls_loss', data=df, ax=axs[4,0])\nsns.lineplot(x='epoch', y='val/dfl_loss', data=df, ax=axs[4,1])\n\n# set titles and axis labels for each subplot\naxs[0,0].set(title='Train Box Loss')\naxs[0,1].set(title='Train Class Loss')\naxs[1,0].set(title='Train DFL Loss')\naxs[1,1].set(title='Metrics Precision (B)')\naxs[2,0].set(title='Metrics Recall (B)')\naxs[2,1].set(title='Metrics mAP50 (B)')\naxs[3,0].set(title='Metrics mAP50-95 (B)')\naxs[3,1].set(title='Validation Box Loss')\naxs[4,0].set(title='Validation Class Loss')\naxs[4,1].set(title='Validation DFL Loss')\n\n# add suptitle and subheader\nplt.suptitle('Training Metrics and Loss', fontsize=24)\n\n# adjust top margin to make space for suptitle\nplt.subplots_adjust(top=0.8)\n\n# adjust spacing between subplots\nplt.tight_layout()\n\nplt.show()\n","metadata":{"executionInfo":{"elapsed":981,"status":"ok","timestamp":1679636492184,"user":{"displayName":"Umar Saeed","userId":"03898094718291107188"},"user_tz":-300},"id":"AgxVGspkHvYa","outputId":"5fd099b1-f210-4795-d7cc-e0a556e1d456","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-01T06:47:28.209553Z","iopub.execute_input":"2023-11-01T06:47:28.210739Z","iopub.status.idle":"2023-11-01T06:47:30.607055Z","shell.execute_reply.started":"2023-11-01T06:47:28.210676Z","shell.execute_reply":"2023-11-01T06:47:30.605612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìäEvaluation\n---","metadata":{"id":"as4JR7mAiZOH"}},{"cell_type":"markdown","source":"<div style=\"background-color:#f2f2f2; padding: 20px;\">\n<h2>mAP Metrics</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">Mean Average Precision (mAP) is a popular evaluation metric in object detection, including the YOLO model. It is used to assess the accuracy of an object detection model by measuring how well it can detect objects in an image, as well as the precision of those detections. mAP takes into account both the number of correctly identified objects and the quality of the detections, which means that it is a robust metric for assessing the performance of an object detection model.</p>\n\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">In YOLO, mAP is particularly important because it measures the accuracy of the model in detecting objects of interest. The higher the mAP, the better the model is at identifying objects in an image. Since YOLO is an object detection model designed for real-time applications, achieving high mAP scores is crucial to ensure that the model can accurately detect objects in real-world scenarios. A high mAP score indicates that the model can effectively identify objects and can be used with confidence in real-world applications.</p>\n\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">However, it is worth noting that mAP is not a perfect metric and has its limitations. For example, it does not account for the difficulty of detecting certain types of objects or the importance of different object classes. Nevertheless, it remains a widely used and valuable metric for evaluating object detection models such as YOLO. With its ability to provide a reliable assessment of a model's ability to detect objects, mAP is an essential tool for both researchers and practitioners in the field of computer vision.</p>\n</div>","metadata":{"id":"kMgY-dGSM57w"}},{"cell_type":"code","source":"%matplotlib inline\n# Loading the best performing model\nmodel = YOLO('/kaggle/working/runs/detect/train/weights/best.pt')\n\n# Evaluating the model on the test dataset\nmetrics = model.val(conf = 0.25, split = 'test')","metadata":{"executionInfo":{"elapsed":2687,"status":"ok","timestamp":1679730627819,"user":{"displayName":"Umar Saeed","userId":"03898094718291107188"},"user_tz":-300},"id":"YoJqV-zfvyxn","execution":{"iopub.status.busy":"2023-11-01T06:50:09.863091Z","iopub.execute_input":"2023-11-01T06:50:09.86407Z","iopub.status.idle":"2023-11-01T06:50:22.090568Z","shell.execute_reply.started":"2023-11-01T06:50:09.86403Z","shell.execute_reply":"2023-11-01T06:50:22.089195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n# Create the barplot\nax = sns.barplot(x=['mAP50-95', 'mAP50', 'mAP75'], y=[metrics.box.map, metrics.box.map50, metrics.box.map75])\n\n# Set the title and axis labels\nax.set_title('YOLO Evaluation Metrics')\nax.set_xlabel('Metric')\nax.set_ylabel('Value')\n\n# Set the figure size\nfig = plt.gcf()\nfig.set_size_inches(8, 6)\n\n# Add the values on top of the bars\nfor p in ax.patches:\n    ax.annotate('{:.3f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n    \n# Show the plot\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-11-01T06:50:45.649689Z","iopub.execute_input":"2023-11-01T06:50:45.650866Z","iopub.status.idle":"2023-11-01T06:50:45.920975Z","shell.execute_reply.started":"2023-11-01T06:50:45.650814Z","shell.execute_reply":"2023-11-01T06:50:45.919746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#f2f2f2; padding: 20px;\">\n<h2>Confusion Matrix</h2>\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">Confusion matrix is a useful tool in evaluating the performance of object detection algorithms like YOLO. In object detection, the confusion matrix can be used to calculate various performance metrics like precision, recall, and F1 score. The confusion matrix is a table that summarizes the true positive, true negative, false positive, and false negative predictions made by the model. In the case of Car Damage Assessment using YOLO, the confusion matrix can be used to evaluate the model's performance in detecting cars in aerial images.</p>\n\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">The rows of the confusion matrix represent the ground truth labels (i.e., the actual presence or absence of a car in the image), while the columns represent the predicted labels (i.e., the model's prediction of the presence or absence of a car). The true positives (TP) represent the cases where the model correctly predicts the presence of a car, while the true negatives (TN) represent the cases where the model correctly predicts the absence of a car. The false positives (FP) represent the cases where the model incorrectly predicts the presence of a car when there is none, while the false negatives (FN) represent the cases where the model incorrectly predicts the absence of a car when there is one. By looking at these values, we can calculate various performance metrics that can help us evaluate the model's performance.</p>\n\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; \">Overall, the confusion matrix is a useful tool in evaluating the performance of object detection algorithms like YOLO. By using this tool, we can calculate various performance metrics like precision, recall, and F1 score, which can help us understand how well the model is performing in detecting cars in aerial images. This, in turn, can help us improve the model by identifying areas where it is making mistakes and tweaking the model's architecture and parameters accordingly.</p>\n</div>","metadata":{"id":"NLqjI3h3Mz0D"}},{"cell_type":"code","source":"%matplotlib inline\n# Reading the confusion matrix image file\nimg = mpimg.imread('/kaggle/working/runs/detect/train/confusion_matrix.png')\n\n# Plotting the confusion matrix image\nfig, ax = plt.subplots(figsize = (15, 15))\n\nax.imshow(img)\nax.axis('off');","metadata":{"executionInfo":{"elapsed":2295,"status":"ok","timestamp":1679653740327,"user":{"displayName":"Umar Saeed","userId":"03898094718291107188"},"user_tz":-300},"id":"VSa9b5RFIJ5N","outputId":"0ce60202-c3e5-4ea3-e264-a21261d3f66d","_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-11-01T06:52:01.702777Z","iopub.execute_input":"2023-11-01T06:52:01.703229Z","iopub.status.idle":"2023-11-01T06:52:03.514924Z","shell.execute_reply.started":"2023-11-01T06:52:01.703195Z","shell.execute_reply":"2023-11-01T06:52:03.513352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîÆMaking Predictions on Test Images\n---","metadata":{"id":"4ADFehkXMwoM"}},{"cell_type":"code","source":"# Function to perform car damage detections\ndef car_detect(img_path):\n    \n    # Read the image\n    img = cv2.imread(img_path)\n\n    # Pass the image through the detection model and get the result\n    detect_result = model(img)\n\n    # Plot the detections\n    detect_img = detect_result[0].plot()\n    \n    # Convert the image to RGB format\n    detect_img = cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB)\n    \n    return detect_img","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-11-01T06:52:08.99311Z","iopub.execute_input":"2023-11-01T06:52:08.993966Z","iopub.status.idle":"2023-11-01T06:52:09.002203Z","shell.execute_reply.started":"2023-11-01T06:52:08.993925Z","shell.execute_reply":"2023-11-01T06:52:09.000818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Define the directory where the custom images are stored\ncustom_image_dir = '/kaggle/input/car-damage-detection/test/images'\n\n# Get the list of image files in the directory\nimage_files = os.listdir(custom_image_dir)\n\n# Select 16 random images from the list\nselected_images = random.sample(image_files, 16)\n\n# Create a figure with subplots for each image\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 15))\n\n# Iterate over the selected images and plot each one\nfor i, img_file in enumerate(selected_images):\n    \n    # Compute the row and column index of the current subplot\n    row_idx = i // 4\n    col_idx = i % 4\n    \n    # Load the current image and run object detection\n    img_path = os.path.join(custom_image_dir, img_file)\n    detect_img = car_detect(img_path)\n    \n    # Plot the current image on the appropriate subplot\n    axes[row_idx, col_idx].imshow(detect_img)\n    axes[row_idx, col_idx].axis('off')\n\n# Adjust the spacing between the subplots\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-11-01T06:52:15.373029Z","iopub.execute_input":"2023-11-01T06:52:15.374106Z","iopub.status.idle":"2023-11-01T06:52:19.257219Z","shell.execute_reply.started":"2023-11-01T06:52:15.374038Z","shell.execute_reply":"2023-11-01T06:52:19.254981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Define the directory where the custom images are stored\ncustom_image_dir = '/kaggle/input/car-damage-detection/test/images'\n\n# Get the list of image files in the directory\nimage_files = os.listdir(custom_image_dir)\n\n# Select 16 random images from the list\nselected_images = random.sample(image_files, 4)\n\n# Create a figure with subplots for each image\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n\n# Iterate over the selected images and plot each one\nfor i, img_file in enumerate(selected_images):\n    \n    # Compute the row and column index of the current subplot\n    row_idx = i // 2\n    col_idx = i % 2\n    \n    # Load the current image and run object detection\n    img_path = os.path.join(custom_image_dir, img_file)\n    detect_img = car_detect(img_path)\n    \n    # Plot the current image on the appropriate subplot\n    axes[row_idx, col_idx].imshow(detect_img)\n    axes[row_idx, col_idx].axis('off')\n\n# Adjust the spacing between the subplots\nplt.subplots_adjust(wspace=0.05, hspace=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:55:45.012978Z","iopub.execute_input":"2023-11-01T06:55:45.01392Z","iopub.status.idle":"2023-11-01T06:55:46.604028Z","shell.execute_reply.started":"2023-11-01T06:55:45.013877Z","shell.execute_reply":"2023-11-01T06:55:46.602558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"specific_image_path = '/kaggle/input/car-damage-detection/train/images/000038_jpg.rf.ad613ba5718956d987002298ebfbd60b.jpg'\n\n# Call the car_detect function to perform detection on the specific image\nresult_image = car_detect(specific_image_path)\n\n# Display the result image\nplt.imshow(result_image)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T06:56:36.447795Z","iopub.execute_input":"2023-11-01T06:56:36.448895Z","iopub.status.idle":"2023-11-01T06:56:36.71457Z","shell.execute_reply.started":"2023-11-01T06:56:36.44885Z","shell.execute_reply":"2023-11-01T06:56:36.7133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\">\n            Thanks for viewing my work. If you like it, consider sharing it to others or give feedback to improve the notebook. Have a beautiful day my friend.\n        </p>\n    </div>\n\n<center><img src='https://media4.giphy.com/media/M9gbBd9nbDrOTu1Mqx/giphy.gif?cid=790b7611704aa2ca4e403287801480a6c753abf45f3e6242&rid=giphy.gif&ct=s' \n     height=30px width=160px /></center>","metadata":{}}]}
